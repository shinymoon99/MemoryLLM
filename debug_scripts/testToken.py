from modeling_memoryllm import MemoryLLM
from configuration_memoryllm import MemoryLLMConfig
from transformers import AutoTokenizer

# load pretrained model
# model = MemoryLLM.from_pretrained("YuWangX/memoryllm-8b")
# tokenizer = AutoTokenizer.from_pretrained("YuWangX/memoryllm-8b")
# Load model and tokenizer from the local directory
model = MemoryLLM.from_pretrained("../memoryllm-8b")
tokenizer = AutoTokenizer.from_pretrained("../memoryllm-8b")

model = model.bfloat16()
model.config._attn_implementation = 'flash_attention_2'
model = model.cuda()



# # Self-Update with the new context
# ctx = "Last week, John had a wonderful picnic with David. During their conversation, David mentioned multiple times that he likes eating apples. Though he didn't mention any other fruits, John says he can infer that David also like bananas."

# # please make sure the context to inject into the memory is larger than 16 tokens, this is the hard minimum when training the model. The memory will be disturbed when less than 16 tokens are injected into the memory. 
# model.inject_memory(tokenizer(ctx, return_tensors='pt', add_special_tokens=False).input_ids.cuda(), update_memory=True)
# inputs = tokenizer("Question: What fruits does David like? Answer: David likes", return_tensors='pt', add_special_tokens=False).input_ids.cuda()
# outputs = model.generate(input_ids=inputs, max_new_tokens=20)
# response = tokenizer.decode(outputs[0][inputs.shape[1]:])
# print("response:",response)

# 猜想1:给定相应功能描述知识的时候，模型能根据这些知识激活本身coding知识并结合提供的知识加以应用(COT)。
# Self-Update with the new context
ctx = """

"""
input_example = """
###code snippet: import argparse\nimport random\nimport numpy as np\nfrom src.models import *\nfrom src.label_propagation import *\nfrom src.plsr import *\nfrom src.knn import *\nfrom src.modabs import *\nfrom src.feature_data import *\nfrom src.multiprototype import *\nfrom src.utils import *\nfrom typing import List\nimport time\nfrom torch.utils.data import random_split\nimport torch\nfrom ray import tune\nimport scipy.stats as stats\n\ndef _parse_args():\n    \"\"\"\n    Command-line arguments to the system. --model switches between the main modes you'll need to use. The other arguments\n    are provided for convenience.\n    :return: the parsed args bundle\n    \"\"\"\n    parser = argparse.ArgumentParser(description='main.py')\n    \n    # General system running and configuration options\n    parser.add_argument('--do_dumb_thing', dest='do_dumb_thing', default=False, action='store_true', help='run the nearest neighbor model')\n    parser.add_argument('--train_data', type=str, default='all', help='mc_rae_real mc_rae_subset buchanan binder (TODO mc_rae_animals) (TODO vinson_vigliocco)')\n    parser.add_argument('--print_dataset', dest='print_dataset', default=False, action='store_true', help=\"Print some sample data on loading\")\n    parser.add_argument('--model', type=str, default='ffnn', help='ffnn (binary) frequency modabs label_propagation knn plsr')\n    parser.add_argument('--layer', type=int, default=8, help='layer of BERT embeddings to use')\n    parser.add_argument('--clusters', type=int, default=1, help='number of lexical prototypes in each BERT multi-prototype embedding')\n    parser.add_argument('--save_path', type=str, default=None, help='path for saving model')\n    parser.add_argument('--embedding_type', type=str, default='bert', help='glove word2vec bert')\n\n    parser.add_argument('--k_fold', dest='k_fold', type=int, default=False, help='train using k-fold cross validation')\n    parser.add_argument('--dev_equals_train', dest='dev_equals_train', default=False, action='store_true', help='use the training words as dev set for debug')\n    parser.add_argument('--allbuthomonyms', dest='allbuthomonyms', default=False, action='store_true', help='train on all available words except for homonyms')\n\n    parser.add_argument('--tuning', default=False, action='store_true', help=\"writes stats to tuning file; does not save trained model\")\n    parser.add_argument('--zscore', default=False, action='store_true', help=\"zscore postprocessing on embeddings\")\n\n\n    add_models_args(parser) # defined in models.py\n\n    args = parser.parse_args()\n\n\n    return args\n\ndef predict_write_output_to_file(exs: List[FeatureNorm], classifier, outfile: str):\n    \"\"\"\n    Runs prediction on exs and writes the outputs to outfile, one token per line\n    :param exs:\n    :param classifier:\n    :param outfile:\n    :return:\n    \"\"\"\n    f = open(outfile, 'w')\n    for ex in exs:\n        for idx in range(0, len(ex)):\n            prediction = classifier.predict(ex, idx)\n            f.write(ex.tokens[idx] + \" \" + repr(int(prediction)) + \"\\n\")\n        f.write(\"\\n\")\n    f.close()\n\ndef prepare_data(feature_norms: FeatureNorms, embeddings: MultiProtoTypeEmbeddings, args):\n    words = list(feature_norms.vocab.keys())\n\n    if args['allbuthomonyms']:\n        ambiguous_pairs = feature_norms.ambiguous_pairs\n\n        print(\"training on all words but evaluation homonyms\")\n        eval_words = [item for t in ambiguous_pairs for item in t]\n\n        print(\"Starting with %s words\" % len(words))\n        train = [i for i in words if i not in eval_words]\n        random.shuffle(train)\n        val = eval_words\n        test = eval_words\n\n        print(\"Ending up with %s training words\" % len(train))\n\n    else:\n        validation_split = .1\n        random_seed = 42\n\n        dataset_size = len(words)\n        split = int(np.floor(validation_split * dataset_size))\n        val, test, train = random_split(words, [split, split, dataset_size - split * 2 ], generator=torch.Generator().manual_seed(random_seed))\n        print(\"Starting with %s training words\" % len(train))\n\n    return (train, val, test)\n\n\ndef kfold_split(feature_norms: FeatureNorms, embeddings: MultiProtoTypeEmbeddings, k):\n    random_seed = 42\n    words = list(feature_norms.vocab.keys())\n    dataset_size = len(words) \n    print(\"starting with dataset of size: \", dataset_size)\n    size_of_chunk = int(np.floor(dataset_size / k))\n\n\n    sizes_of_chunks = [size_of_chunk for i in range(k)]\n\n    # the last chunk might be a little bit bigger if the dataset isnt evenlt divisible\n    last_chunk_size = dataset_size - (size_of_chunk * (k-1))\n    sizes_of_chunks[-1] = last_chunk_size\n\n    chunks = random_split(words, sizes_of_chunks, generator=torch.Generator().manual_seed(random_seed))\n\n    return chunks\n\n\ndef load_feature_norms(args):\n    if args['train_data'] == 'mc_rae_real':\n        print(\"gets here\")\n        feature_norms = McRaeFeatureNorms('./data/external/mcrae/CONCS_FEATS_concstats_brm/concepts_features-Table1.csv')\n    elif args['train_data'] == 'mc_rae_subset':\n        feature_norms = BuchananFeatureNorms('data/external/buchanan/cue_feature_words.csv', subset='mc_rae_subset')\n    elif args['train_data'] == 'buchanan':\n        feature_norms = BuchananFeatureNorms('data/external/buchanan/cue_feature_words.csv')\n    elif args['train_data'] == 'binder':\n        feature_norms = BinderFeatureNorms('data/external/binder_word_ratings/WordSet1_Ratings.csv')\n    else:\n        raise Exception(\"dataset not implemented\")\n    return feature_norms\n\ndef load_embeddings(args):\n    if args['embedding_type'] == 'bert':\n        embedding_file = './data/processed/multipro_embeddings/layer'+ str(args['layer']) + 'clusters' + str(args['clusters']) + '.txt'\n        embs = read_multiprototype_embeddings(embedding_file, layer=args['layer'], num_clusters=args['clusters'])\n\n    elif args['embedding_type'] == 'glove':\n        embeddings_list = []\n        word_indexer = Indexer()\n        with open(\"data/external/glove.6B/glove.6B.300d.txt\", 'r') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                vector = np.<token_mask>(values[1:], \"float32\")\n                embeddings_list.append([vector])\n                word_indexer.add_and_get_index(word)\n\n        embs = MultiProtoTypeEmbeddings(word_indexer, np.array(embeddings_list), 0, 1) # dummy layer, clusters = 1\n    if args['zscore']:\n        print(\"training on zscore-normalized embeddings\")\n        reshape = embs.vectors.reshape(61740, 768)\n        zscored = stats.zscore(reshape, axis=0)\n        rereshape = zscored.reshape(embs.vectors.shape[0], 5, 768)\n        embs.vectors = rereshape\n\n    return embs\n\n\n\n\ndef train(train_words, dev_words, test_words, embs, feature_norms, args):\n    start = time.time()\n    if args['do_dumb_thing']:\n        model = FrequencyClassifier(feature_norms)\n    elif args['model'] == 'ffnn':\n        model = train_ffnn(train_words, dev_words, embs, feature_norms, args)\n    elif args['model'] == 'label_propagation':\n        model = train_label_propagation(train_words, dev_words, embs, feature_norms, args)\n    elif args['model'] == 'knn':\n        model = train_knn_regressor(train_words, dev_words, embs, feature_norms, args)\n    elif args['model'] == 'plsr':\n        model = train_plsr(train_words, dev_words, embs, feature_norms, args)\n    elif args['model'] == 'modabs':\n        model = train_mad(train_words, dev_words, test_words, embs, feature_norms, args)\n    else:\n        raise Exception(\"model not implemented: \", args.model)\n\n    end = time.time()\n    print(\"Time elapsed during training: %s seconds\" % (end - start))\n    return model\n\ndef train_1_fold(feature_norms, embs, args):\n\n    train_words, dev_words, test_words = prepare_data(feature_norms, embs, args)\n\n    \"\"\"\n    for debug we might want the dev set to be the trains et just to see if we're learning those\n    \"\"\"\n    if args['dev_equals_train']:\n        dev_words = train_words\n\n    print(\"%i train exs, %i dev exs, %i test exs\" % (len(train_words), len(dev_words), len(test_words)))\n\n\n    model = train(train_words, dev_words, test_words, embs, feature_norms, args)\n\n    print(\"=======DEV SET=======\")\n    results = evaluate(model, dev_words, feature_norms, args, debug='false')\n    print(results)\n    tune.report(\n        dev_MAP_at_10=results['MAP_at_10'],\n        dev_MAP_at_20=results['MAP_at_20'],\n        dev_MAP_at_k=results['MAP_at_k'],\n        dev_correl=results['correl'],\n        dev_cos=results['cos'],\n        dev_rsquare=results['rsquare'],\n        dev_mse=results['mse']\n    )\n\n    print(\"=======FINAL PRINTING ON TEST SET=======\")\n    print(\"testing on these words:\")\n    for test_word in test_words:\n        print(test_word)\n    results = evaluate(model, test_words, feature_norms, args, debug='false')\n    tune.report(\n        test_MAP_at_10=results['MAP_at_10'],\n        test_MAP_at_20=results['MAP_at_20'],\n        test_MAP_at_k=results['MAP_at_k'],\n        test_correl=results['correl'],\n        test_cos=results['cos'],\n        test_rsquare=results['rsquare'],\n        test_mse=results['mse']\n    )\n\n    if (args['save_path'] is not None):\n        if not args['tuning']:\n            torch.save(model, args['save_path'])\n            print(\"Wrote trained model to \", args['save_path'])   \n\ndef kfold_crossvalidation(feature_norms, embs, args):\n    k = args[\"k_fold\"]\n\n    chunks = kfold_split(feature_norms, embs, k)\n\n    # split data into 10 equal parts, and then iterate training 10 times.\n    dev_stats = []\n    test_stats = []\n    for i in range(0,k):\n\n        print(\" \")\n        print(\" \")\n        print(\"*------------------------------------------------------------*\")\n        print(\"Running k-fold cross validation, k=%s, this is iteration %s\" % (k, i))\n        # take the first two off and use them\n        test_words = chunks.pop(0)\n        dev_words = chunks[0]\n        # flatten the rest of the list (the k-2 folds) to use as training data. \n        # for k = 10, this takes 8 equal sized lists and makes them one list\n        train_words = [item for sublist in chunks[1:] for item in sublist]\n        # add the test words back to the end of the list (we took them from the beginning of th elist)\n        chunks.append(test_words)\n\n        print(\"size of train set:\", len(train_words))\n        print(\"size of dev set:\", len(dev_words))\n        print(\"size of test set:\", len(test_words))\n\n\n        print(\"%i train exs, %i dev exs, %i test exs\" % (len(train_words), len(dev_words), len(test_words)))\n\n        model = train(train_words, dev_words, test_words, embs, feature_norms, args)\n\n\n\n        # always get dev set results\n        results = evaluate(model, dev_words, feature_norms, args, debug='false')\n        dev_stats.append(results)\n\n        # if we are doing final eval get test set results\n        #if not args['tuning']:\n        results = evaluate(model, test_words, feature_norms, args, debug='false')\n        test_stats.append(results)\n\n\n    print(\"=======DEV SET=======\")\n    all_folds = pd.DataFrame.from_records(dev_stats)\n    print(dev_stats)\n    average = all_folds.mean(axis=0)\n    tune.report(\n        dev_MAP_at_10=average.MAP_at_10,\n        dev_MAP_at_20=average.MAP_at_20,\n        dev_MAP_at_k=average.MAP_at_k,\n        dev_correl=average.correl,\n        dev_cos=average.cos,\n        dev_rsquare=average.rsquare,\n        dev_mse=average.mse\n    )\n    print(average)\n\n    # if we are doing final eval get test set results\n    #if not args['tuning']:\n    print(\"=======FINAL PRINTING ON TEST SET=======\")\n    all_folds = pd.DataFrame.from_records(test_stats)\n    average = all_folds.mean(axis=0)\n    tune.report(\n        test_MAP_at_10=average.MAP_at_10,\n        test_MAP_at_20=average.MAP_at_20,\n        test_MAP_at_k=average.MAP_at_k,\n        test_correl=average.correl,\n        test_cos=average.cos,\n        test_rsquare=average.rsquare,\n        test_mse=average.mse\n    )\n    print(average)\n\n\ndef main(args):\n\n    print(args)\n\n    os.chdir(args['TUNE_ORIG_WORKING_DIR'])\n\n    random.seed(args['seed'])\n    np.random.seed(args['seed'])\n    print(args['train_data'])\n    feature_norms = load_feature_norms(args)\n    embs = load_embeddings(args)\n\n\n    if args['k_fold']:\n        kfold_crossvalidation(feature_norms, embs, args)\n    else:\n        train_1_fold(feature_norms, embs, args)\n\n\nif __name__ == '__main__':\n    args = _parse_args()\n    args = vars(args)\n\n    args['TUNE_ORIG_WORKING_DIR'] = os.getcwd()\n    tune.run(main, config=args)\n ###Function Description: The code defines a system for running different training models on a dataset. It includes functionalities such as parsing command-line arguments, loading feature norms and embeddings, preparing data splits (including k-fold cross-validation), training different models like FFNN, label propagation, KNN regressor, PLSR, and MAD. It also includes functions for prediction, writing output to a file, and evaluating model performance on test data. The system can be used for training, evaluating, and tuning different models on the provided dataset. 
"""
input_untokenized =f"""

You are a professional Python programming engineer, and I will give you a code snippet where function names are masked and represented as<mask>in the code. There may be multiple <mask>, and all the blocked content in these <mask> is the same. I will provide a functional description of this code, the dependency package to which the function belongs and the version of the dependency package. What you need to do is infer what the masked function name is based on this information. You only need to return one content, not every<mask>. Please note that you only need to return one function name and do not need to return any other redundant content, and the response is enclosed by <start> and <end>.Here is an example: ###code snippet: outputs = llm.<token_mask>(prompts, sampling_params) ###Function Description: This code passes prompts and parameters to the model to obtain the output result of the model. ###dependency and version: vllm==0.3.3 ###response: <start>generate<end>.so, given above example,what should should be fit into <token_mask> in below example:vector = np.<token_mask>(values[1:], \"float32\").{input_example}###dependency and version: numpy==1.24.2 ###response:
"""
# please make sure the context to inject into the memory is larger than 16 tokens, this is the hard minimum when training the model. The memory will be disturbed when less than 16 tokens are injected into the memory. 
model.inject_memory(tokenizer(ctx, return_tensors='pt', add_special_tokens=False).input_ids.cuda(), update_memory=True)
inputs = tokenizer(input_untokenized, return_tensors='pt', add_special_tokens=False).input_ids.cuda()
outputs = model.generate(input_ids=inputs, max_new_tokens=16)
response = tokenizer.decode(outputs[0][inputs.shape[1]:])
print("response:",response)