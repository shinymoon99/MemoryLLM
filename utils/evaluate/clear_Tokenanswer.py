"""
Clear the <start> and <end> generated by the model in inference
"""

import json
import os

model_name = 'Llama-3-70b-chat-hf'

result_path = 'output/versicodeOutput_token_dac.jsonl'

data_list = []

# Read the JSONL file
with open(result_path, 'r', encoding='utf-8') as fr:
    for line in fr:
        data_list.append(json.loads(line))
# convert pred element to model_output
for data in data_list:
    data['model_output'] = data['pred']

for data in data_list:
    temp_list = []
    model_output_list = [data['model_output']]  # This should now be a list of strings

    for output in model_output_list:
        if "<start>" in output and "<end>" in output:
            start_index = output.find("<start>") + len("<start>")
            end_index = output.find("<end>")
            content = output[start_index:end_index].replace('```python', '').replace('```', '')
        else:
            content = "no_answer"

        temp_list.append(content)

    data['model_output_token_clear'] = str(temp_list)

# Write the modified data back to the file
with open(result_path, 'w', encoding='utf-8') as fw:
    for data in data_list:
        json.dump(data, fw)
        fw.write('\n')

print(f"Processed data has been written back to {result_path}")